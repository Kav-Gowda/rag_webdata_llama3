# -*- coding: utf-8 -*-
"""rag_webdata_llama3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kwVq-9qbjQ2chJepTHq_UpR90b9tvbOa
"""

# Commented out IPython magic to ensure Python compatibility.
# --- Environment Setup ---
# %pip install langchain==0.2.6
# %pip install langchain_chroma==0.1.2
# %pip install langchain-community==0.2.6
# %pip install ibm-watsonx-ai==1.0.10
# %pip install langchain_ibm==0.1.11
# %pip install unstructured==0.15.0
# %pip install ibm-watson-machine-learning==1.0.360

# --- Imports and Basic Setup ---
import os
import warnings
warnings.filterwarnings('ignore')

from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes

from langchain_ibm import WatsonxEmbeddings, WatsonxLLM
from langchain.vectorstores import Chroma

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- IBM watsonx.ai Credentials Setup ---

from ibm_watsonx_ai import Credentials
import os

# NOTE: Replace with your own watsonx.ai instance URL and project ID when running locally
credentials = Credentials(
    url="your_watsonx_instance_url"
)

project_id = "your_project_id"

print("watsonx.ai credentials initialized successfully.")

# --- Load Web Documents ---

import requests

class Document:
    def __init__(self, metadata, page_content):
        self.metadata = metadata
        self.page_content = page_content

# Example dictionary of web pages to load
URLS_DICTIONARY = {
    "example_page_1": "https://your-website-url-1.com/sample.txt",
    "example_page_2": "https://your-website-url-2.com/sample.txt",
}

COLLECTION_NAME = "web_documents"

documents = []

for name, url in URLS_DICTIONARY.items():
    print(f"Loading from {url}")
    response = requests.get(url)

    if response.status_code == 200:
        data = {
            "metadata": {"source": url, "name": name},
            "page_content": response.text
        }
        documents.append(Document(metadata=data["metadata"], page_content=data["page_content"]))
        print(f"Loaded from {url}")
    else:
        print(f"Failed to retrieve content from {url}")

print("‚úÖ Sample Document Metadata:", documents[0].metadata)
print("üìÑ Sample Document Content:\n", documents[0].page_content[:500])  # show first 500 chars

# --- Clean, Split, Embed, and Store Web Documents ---

from IPython.display import display
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1Ô∏è‚É£ Clean documents and assign unique IDs
for i, doc in enumerate(documents):
    doc.page_content = " ".join(doc.page_content.split())  # remove extra whitespace
    doc.metadata["id"] = i
    print(f"Processed document {i}: {doc.metadata}")

# Preview a sample document
display(documents[0].metadata)
print("üìÑ Sample content:\n", documents[0].page_content[:500], "\n")

# 2Ô∏è‚É£ Split documents into smaller chunks for better retrieval
text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
print(f"‚úÖ Documents split into {len(docs)} chunks.")

# 3Ô∏è‚É£ Create embeddings using Watsonx Embeddings
embeddings = WatsonxEmbeddings(
    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,
    url=credentials["url"],
    project_id=project_id
)
print("‚úÖ Embeddings initialized.")

# 4Ô∏è‚É£ Build a Chroma vector store
vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)
print("‚úÖ Chroma vector store created and populated.")

# 5Ô∏è‚É£ Quick retrieval test
query = "What is _____?"
results = vectorstore.similarity_search_with_score(query, k=4)

print(f"\nüß† Query: {query}")
for i, (doc, score) in enumerate(results, start=1):
    print(f"\nResult {i} ‚Äî Score: {score:.3f}")
    print(doc.page_content[:300])

# --- Create Retriever for Semantic Search ---

retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
print("‚úÖ Retriever initialized (top 2 results per query).")

# --- Configure Llama 3.1, build chain, and run sample queries ---

# Model + params
model_id = "meta-llama/llama-3-405b-instruct"
parameters = {
    GenTextParamsMetaNames.DECODING_METHOD: "greedy",
    GenTextParamsMetaNames.MIN_NEW_TOKENS: 10,
    GenTextParamsMetaNames.MAX_NEW_TOKENS: 512,
    GenTextParamsMetaNames.REPETITION_PENALTY: 1,
    GenTextParamsMetaNames.RETURN_OPTIONS: {
        "input_tokens": True,
        "generated_tokens": True,
        "token_logprobs": True,
        "token_ranks": True,
    },
}

# LLM (placeholders kept for security)
llm = WatsonxLLM(
    model_id=model_id,
    url="your_watsonx_instance_url",
    apikey="your_ibm_watsonx_api_key",
    project_id=project_id,
    params=parameters,
)

# Prompt
template = """Generate a concise answer grounded in the provided context. Explain in steps if helpful.
Answer style should match the context. Ideal length: 2‚Äì3 sentences.

{context}
Question: {question}
Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

# Format retrieved docs into a single context string
def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

# Chain: retrieve ‚Üí prompt ‚Üí LLM ‚Üí parse
chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Run neutral sample queries (no brand-specific wording)
import pprint
for q in [
    "Give a concise overview of the main topic covered in these pages.",
    "List two or three key takeaways supported by the content.",
]:
    print("\nQ:", q)
    ans = chain.invoke(q)
    pprint.pprint(ans, width=120)